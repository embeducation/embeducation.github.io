// @tensorflow/tfjs-models Copyright 2019 Google
import{util,loadGraphModel,tensor2d,tensor1d}from"@tensorflow/tfjs";function __awaiter(e,n,r,t){return new(r||(r=Promise))(function(o,i){function a(e){try{u(t.next(e))}catch(e){i(e)}}function s(e){try{u(t.throw(e))}catch(e){i(e)}}function u(e){e.done?o(e.value):new r(function(n){n(e.value)}).then(a,s)}u((t=t.apply(e,n||[])).next())})}function __generator(e,n){var r,t,o,i,a={label:0,sent:function(){if(1&o[0])throw o[1];return o[1]},trys:[],ops:[]};return i={next:s(0),throw:s(1),return:s(2)},"function"==typeof Symbol&&(i[Symbol.iterator]=function(){return this}),i;function s(i){return function(s){return function(i){if(r)throw new TypeError("Generator is already executing.");for(;a;)try{if(r=1,t&&(o=2&i[0]?t.return:i[0]?t.throw||((o=t.return)&&o.call(t),0):t.next)&&!(o=o.call(t,i[1])).done)return o;switch(t=0,o&&(i=[2&i[0],o.value]),i[0]){case 0:case 1:o=i;break;case 4:return a.label++,{value:i[1],done:!1};case 5:a.label++,t=i[1],i=[0];continue;case 7:i=a.ops.pop(),a.trys.pop();continue;default:if(!(o=(o=a.trys).length>0&&o[o.length-1])&&(6===i[0]||2===i[0])){a=0;continue}if(3===i[0]&&(!o||i[1]>o[0]&&i[1]<o[3])){a.label=i[1];break}if(6===i[0]&&a.label<o[1]){a.label=o[1],o=i;break}if(o&&a.label<o[2]){a.label=o[2],a.ops.push(i);break}o[2]&&a.ops.pop(),a.trys.pop();continue}i=n.call(e,a)}catch(e){i=[6,e],t=0}finally{r=o=0}if(5&i[0])throw i[1];return{value:i[0]?i[1]:void 0,done:!0}}([i,s])}}}var stringToChars=function(e){for(var n=[],r=0,t=e;r<t.length;r++){var o=t[r];n.push(o)}return n},TrieNode=function(){function e(e){this.key=e,this.parent=null,this.children={},this.end=!1}return e.prototype.getWord=function(){for(var e=[],n=this;null!==n;)null!==n.key&&e.unshift(n.key),n=n.parent;return[e,this.score,this.index]},e}(),Trie=function(){function e(){this.root=new TrieNode(null)}return e.prototype.findAllCommonPrefixes=function(e,n,r){if(n.end){var t=n.getWord();e.slice(0,t[0].length).join("")===t[0].join("")&&r.unshift(t)}for(var o in n.children)this.findAllCommonPrefixes(e,n.children[o],r)},e.prototype.insert=function(e,n,r){for(var t=this.root,o=stringToChars(e),i=0;i<o.length;i++)t.children[o[i]]||(t.children[o[i]]=new TrieNode(o[i]),t.children[o[i]].parent=t),t=t.children[o[i]],i===o.length-1&&(t.end=!0,t.score=n,t.index=r)},e.prototype.commonPrefixSearch=function(e){var n=this.root.children[e[0]],r=[];return n?this.findAllCommonPrefixes(e,n,r):r.push([[e[0]],0,0]),r},e}(),separator="â–";function processInput(e){var n=e.normalize("NFKC");return separator+n.replace(/ /g,separator)}var RESERVED_SYMBOLS_COUNT=6,Tokenizer=function(){function e(e){this.vocabulary=e,this.trie=new Trie;for(var n=RESERVED_SYMBOLS_COUNT;n<this.vocabulary.length;n++)this.trie.insert(this.vocabulary[n][0],this.vocabulary[n][1],n)}return e.prototype.encode=function(e){var n=[],r=[],t=[];e=processInput(e);for(var o=stringToChars(e),i=0;i<=o.length;i++)n.push({}),r.push(0),t.push(0);for(i=0;i<o.length;i++)for(var a=this.trie.commonPrefixSearch(o.slice(i)),s=0;s<a.length;s++){var u=a[s],l={key:u[0],score:u[1],index:u[2]};null==n[i+(c=u[0].length)][i]&&(n[i+c][i]=[]),n[i+c][i].push(l)}for(var c=0;c<=o.length;c++)for(var h in n[c]){var f=n[c][h];for(s=0;s<f.length;s++){var d=f[s],p=d.score+t[c-d.key.length];(0===t[c]||p>=t[c])&&(t[c]=p,r[c]=f[s].index)}}for(var v=[],g=r.length-1;g>0;)v.push(r[g]),g-=this.vocabulary[r[g]][0].length;var y=[],_=!1;for(i=0;i<v.length;i++){var b=v[i];_&&0===b||y.push(b),_=0===b}return y.reverse()},e}(),BASE_PATH="https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder/";function load(){return __awaiter(this,void 0,void 0,function(){var e;return __generator(this,function(n){switch(n.label){case 0:return[4,(e=new UniversalSentenceEncoder).load()];case 1:return n.sent(),[2,e]}})})}function loadTokenizer(e){return __awaiter(this,void 0,void 0,function(){var n;return __generator(this,function(r){switch(r.label){case 0:return[4,loadVocabulary(e)];case 1:return n=r.sent(),[2,new Tokenizer(n)]}})})}function loadVocabulary(e){return void 0===e&&(e=BASE_PATH+"vocab.json"),__awaiter(this,void 0,void 0,function(){return __generator(this,function(n){switch(n.label){case 0:return[4,util.fetch(e)];case 1:return[2,n.sent().json()]}})})}var UniversalSentenceEncoder=function(){function e(){}return e.prototype.loadModel=function(){return __awaiter(this,void 0,void 0,function(){return __generator(this,function(e){return[2,loadGraphModel(BASE_PATH+"model.json")]})})},e.prototype.load=function(){return __awaiter(this,void 0,void 0,function(){var e,n,r;return __generator(this,function(t){switch(t.label){case 0:return[4,Promise.all([this.loadModel(),loadVocabulary()])];case 1:return e=t.sent(),n=e[0],r=e[1],this.model=n,this.tokenizer=new Tokenizer(r),[2]}})})},e.prototype.embed=function(e){return __awaiter(this,void 0,void 0,function(){var n,r,t,o,i,a,s,u=this;return __generator(this,function(l){switch(l.label){case 0:for("string"==typeof e&&(e=[e]),n=e.map(function(e){return u.tokenizer.encode(e)}),r=n.map(function(e,n){return e.map(function(e,r){return[n,r]})}),t=[],o=0;o<r.length;o++)t=t.concat(r[o]);return i=tensor2d(t,[t.length,2],"int32"),a=tensor1d(util.flatten(n),"int32"),[4,this.model.executeAsync({indices:i,values:a})];case 1:return s=l.sent(),i.dispose(),a.dispose(),[2,s]}})})},e}();export{load,loadTokenizer,UniversalSentenceEncoder,Tokenizer};
